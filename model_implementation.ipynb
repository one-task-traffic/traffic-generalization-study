{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "import faiss\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import xgboost as xgb\n",
    "\n",
    "import tensorflow as tf\n",
    "from scipy import stats\n",
    "from keras.callbacks import CSVLogger, TensorBoard, ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from utils.utils import *\n",
    "from utils.data_generator import *\n",
    "from utils.cesnetpreprocessing import generate_balanced_label_and_ID_list_for_labels, load_data_for_ids\n",
    "from utils.config import get_all_labelled_IDs, get_both_label_indices, get_file_startIds, map_label_num_to_soori_label, picked_fine_classes_25, fine2coarse, CUTOFF_POINT\n",
    "from models.Model_First import AttentionModelTrunk, build_full_model as build_first_model\n",
    "from models.Model_Full import build_model as build_full_model\n",
    "from models.Model_NewRun import build_model as build_newrun_model\n",
    "from models.Model_NoMB import build_model as build_nomb_model\n",
    "from models.Model_MoreBlocks import AttentionModelTrunk, build_full_model as build_moreblocks_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "dim = CUTOFF_POINT\n",
    "n_channels = 3\n",
    "n_classes = len(picked_fine_classes_25)\n",
    "notebook_output_prefix = './model_outputs/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "both_label_indices = get_both_label_indices()\n",
    "all_labelled_IDs = get_all_labelled_IDs()\n",
    "file_startIds = get_file_startIds()\n",
    "day_3_endID = int(file_startIds[3][1])\n",
    "training_x, training_y, perlabel_training_x, perlabel_num = generate_balanced_label_and_ID_list_for_labels(0, day_3_endID, 1000000, \n",
    "                                        picked_fine_classes_25, 0, both_label_indices, return_coarse=False)\n",
    "actual_training_y = [map_label_num_to_soori_label(y) for y in training_y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indices, val_indices = train_test_split(range(len(training_x)), test_size = 0.20, random_state=19)\n",
    "\n",
    "training_samples = list(zip(training_x, actual_training_y))\n",
    "training_exclu_x = [ training_samples[ind][0] for ind in train_indices]\n",
    "training_exclu_y = [ training_samples[ind][1] for ind in train_indices]\n",
    "\n",
    "val_x = [ training_samples[ind][0] for ind in val_indices]\n",
    "val_y = [ training_samples[ind][1] for ind in val_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The First Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First Model aka Second Run\n",
    "MODEL_NAME = \"FirstModel\"\n",
    "path_prefix = f'{notebook_output_prefix}{MODEL_NAME}'\n",
    "\n",
    "attention_model = AttentionModelTrunk(name='FirstModelTrunk',\n",
    "                             num_heads=1, head_size=3, ff_dim=64, num_layers=2, dropout=0.1)\n",
    "model_first = build_first_model((30,3), attention_model, n_classes, [512], 0.01, 0.01)\n",
    "\n",
    "training_generator, validation_generator = get_data_generator(training_exclu_x, training_exclu_y, val_x, val_y, n_classes, \n",
    "                                   norm_func=normalize(\"minmax\", \"cesnet\"),feature_select=[0,1,2],\n",
    "                                   batch_size = 32, dim =(30), n_channels =3)\n",
    "\n",
    "dummy_init_lr = 1e-8\n",
    "model_first.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=dummy_init_lr),\n",
    "    metrics=['accuracy',tf.keras.metrics.Precision(),\n",
    "             tf.keras.metrics.Recall()]\n",
    "    )\n",
    "\n",
    "epoch_list = [1, 2, 4, 8, 8]\n",
    "scheduler2 = scheduler2_factory(epoch_list, [1e-3, 1e-4, 1e-5, 1e-6, 1e-7])\n",
    "\n",
    "lr_callback = keras.callbacks.LearningRateScheduler(scheduler2)\n",
    "tensorboard = TensorBoard(log_dir=f'{notebook_output_prefix}logs/', histogram_freq=1, write_graph=True,\n",
    "                          write_images=True)\n",
    "checkpointer_loss = ModelCheckpoint(filepath= path_prefix + '_loss.weights.h5', verbose=1, save_best_only=True, save_weights_only=True)\n",
    "checkpointer_acc = ModelCheckpoint(monitor='val_accuracy', filepath= path_prefix+ '_acc.weights.h5', verbose=1, save_best_only=True, save_weights_only=True)\n",
    "tensorboard.set_model(model_first)\n",
    "\n",
    "history = model_first.fit(training_generator, \n",
    "          epochs=sum(epoch_list),\n",
    "          verbose=1,\n",
    "          shuffle=False,\n",
    "          validation_data=validation_generator,\n",
    "          callbacks=[tensorboard,checkpointer_loss,checkpointer_acc,lr_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_first.save_weights(f\"{notebook_output_prefix}/model_weights/{MODEL_NAME}.weights.h5\")\n",
    "model_first.save(f\"{notebook_output_prefix}/full_model/{MODEL_NAME}.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Full Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full Model\n",
    "MODEL_NAME = \"FullModel\"\n",
    "path_prefix = f'{notebook_output_prefix}{MODEL_NAME}'\n",
    "\n",
    "model_full = build_full_model(\n",
    "    (CUTOFF_POINT,3),\n",
    "    head_size=256,\n",
    "    num_heads=8,\n",
    "    ff_dim=64,\n",
    "    num_transformer_blocks=2,\n",
    "    mlp_units=[ 512 ],\n",
    "    mlp_dropout=0.1,\n",
    "    dropout=0.1,\n",
    "    transformer_input_size=256,\n",
    "    n_classes=n_classes\n",
    ")\n",
    "\n",
    "training_generator, validation_generator = get_data_generator(training_exclu_x, training_exclu_y, val_x, val_y, n_classes, \n",
    "                                   norm_func = normalize(\"all-three-n2\", \"cesnet\"), feature_select=[0,1,2],\n",
    "                                   batch_size = 64, dim = (30), n_channels = 3)\n",
    "\n",
    "dummy_init_lr = 1e-8\n",
    "model_full.compile(loss='categorical_crossentropy', \n",
    "                          optimizer=tf.keras.optimizers.Adam(learning_rate=dummy_init_lr), \\\n",
    "                          metrics=['accuracy',tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])\n",
    "\n",
    "epoch_list = [5, 4]\n",
    "scheduler2 = scheduler2_factory(epoch_list, [1e-7, 1e-4])\n",
    "\n",
    "lr_callback = keras.callbacks.LearningRateScheduler(scheduler2)\n",
    "tensorboard = TensorBoard(log_dir=f'{notebook_output_prefix}logs/', histogram_freq=1, write_graph=True,\n",
    "                          write_images=True)\n",
    "checkpointer_loss = ModelCheckpoint(monitor='val_loss', filepath= path_prefix + '_loss.weights.h5', verbose=1, save_best_only=True, save_weights_only=True)\n",
    "checkpointer_acc = ModelCheckpoint(monitor='val_accuracy', filepath= path_prefix+ '_acc.weights.h5', verbose=1, save_best_only=True, save_weights_only=True)\n",
    "checkpointer_train_loss = ModelCheckpoint(monitor='loss', filepath= path_prefix + '_trainloss.weights.h5', verbose=1, save_best_only=True, save_weights_only=True)\n",
    "csv_logger = CSVLogger(path_prefix + '_training.log', append=True)\n",
    "tensorboard.set_model(model_full)\n",
    "\n",
    "history1 = model_full.fit(training_generator, \n",
    "          epochs=sum(epoch_list),\n",
    "          verbose=1,\n",
    "          shuffle=False,\n",
    "          validation_data=validation_generator,\n",
    "          callbacks=[tensorboard,checkpointer_loss,checkpointer_acc,checkpointer_train_loss,lr_callback,csv_logger])\n",
    "\n",
    "init_lr = 1e-7\n",
    "lr_callback = keras.callbacks.LearningRateScheduler(scheduler3)\n",
    "model_full.compile(loss='categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(learning_rate=init_lr), \n",
    "                   metrics=['accuracy',tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])\n",
    "\n",
    "history2 = model_full.fit(training_generator, \n",
    "          epochs=31,\n",
    "          verbose=1,\n",
    "          shuffle=False,\n",
    "          validation_data=validation_generator,\n",
    "          callbacks=[tensorboard,checkpointer_loss,checkpointer_acc,checkpointer_train_loss,lr_callback,csv_logger])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_full.save_weights(f\"{notebook_output_prefix}/model_weights/{MODEL_NAME}.weights.h5\")\n",
    "model_full.save(f\"{notebook_output_prefix}/full_model/{MODEL_NAME}.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The New Run Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New Run Model\n",
    "MODEL_NAME = \"NewRunModel\"\n",
    "path_prefix = f'{notebook_output_prefix}{MODEL_NAME}'\n",
    "\n",
    "model_newrun = build_newrun_model(\n",
    "    (CUTOFF_POINT,3),\n",
    "    head_size=3,\n",
    "    num_heads=8,\n",
    "    ff_dim=64,\n",
    "    num_transformer_blocks=2,\n",
    "    mlp_units=[ 90 ],\n",
    "    mlp_dropout=0.05,\n",
    "    dropout=0.1,\n",
    "    transformer_input_size=3,\n",
    "    n_classes=n_classes\n",
    ")\n",
    "\n",
    "training_generator, validation_generator = get_data_generator(training_exclu_x, training_exclu_y, val_x, val_y, n_classes, \n",
    "                                   norm_func=normalize(\"minmax\", \"cesnet\"),feature_select=[0,1,2],\n",
    "                                   batch_size = 64, dim =(30), n_channels =3)\n",
    "    \n",
    "dummy_init_lr = 1e-8\n",
    "model_newrun.compile(loss='categorical_crossentropy', \n",
    "                          optimizer=tf.keras.optimizers.Adam(learning_rate=dummy_init_lr), \\\n",
    "                          metrics=['accuracy',tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])\n",
    "\n",
    "epoch_list = [5, 5, 10, 10, 30]\n",
    "scheduler2 = scheduler2_factory(epoch_list, [1e-4, 1e-5, 1e-6, 1e-7, 1e-8])\n",
    "\n",
    "lr_callback = keras.callbacks.LearningRateScheduler(scheduler2)\n",
    "tensorboard = TensorBoard(log_dir=f'{notebook_output_prefix}logs/', histogram_freq=1, write_graph=True,\n",
    "                          write_images=True)\n",
    "checkpointer_loss = ModelCheckpoint(monitor='val_loss', filepath= path_prefix + '_loss.weights.h5', verbose=1, save_best_only=True, save_weights_only=True)\n",
    "checkpointer_acc = ModelCheckpoint(monitor='val_accuracy', filepath= path_prefix+ '_acc.weights.h5', verbose=1, save_best_only=True, save_weights_only=True)\n",
    "checkpointer_train_loss = ModelCheckpoint(monitor='loss', filepath= path_prefix + '_trainloss.weights.h5', verbose=1, save_best_only=True, save_weights_only=True)\n",
    "csv_logger = CSVLogger(path_prefix + '_training.log', append=True)\n",
    "tensorboard.set_model(model_newrun)\n",
    "\n",
    "history = model_newrun.fit(training_generator, \n",
    "          epochs=sum(epoch_list),\n",
    "          verbose=1,\n",
    "          shuffle=False,\n",
    "          validation_data=validation_generator,\n",
    "          callbacks=[tensorboard,checkpointer_loss,checkpointer_acc,checkpointer_train_loss,lr_callback,csv_logger])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_newrun.save_weights(f\"{notebook_output_prefix}/model_weights/{MODEL_NAME}.weights.h5\")\n",
    "model_newrun.save(f\"{notebook_output_prefix}/full_model/{MODEL_NAME}.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The NoMB Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NoMB Model\n",
    "MODEL_NAME = \"NoMBModel\"\n",
    "path_prefix = f'{notebook_output_prefix}{MODEL_NAME}'\n",
    "\n",
    "model_nomb = build_nomb_model(\n",
    "    (CUTOFF_POINT,3),\n",
    "    head_size=256,\n",
    "    num_heads=8,\n",
    "    ff_dim=64,\n",
    "    num_transformer_blocks=2,\n",
    "    mlp_units=[ 512 ],\n",
    "    mlp_dropout=0.1,\n",
    "    dropout=0.1,\n",
    "    transformer_input_size=256,\n",
    "    n_classes=n_classes\n",
    ")\n",
    "\n",
    "training_generator, validation_generator = get_data_generator(training_exclu_x, training_exclu_y, val_x, val_y, n_classes, \n",
    "                                   norm_func=normalize(\"minmax\", \"cesnet\"),feature_select=[0,1,2],\n",
    "                                   batch_size = 64, dim =(30), n_channels =3)\n",
    "\n",
    "dummy_init_lr = 1e-8\n",
    "model_nomb.compile(loss='categorical_crossentropy', \n",
    "                          optimizer=tf.keras.optimizers.Adam(learning_rate=dummy_init_lr), \\\n",
    "                          metrics=['accuracy',tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])\n",
    "\n",
    "epoch_list = [4, 4, 2]\n",
    "scheduler2 = scheduler2_factory(epoch_list, [1e-5, 1e-3, 1e-6])\n",
    "\n",
    "lr_callback = keras.callbacks.LearningRateScheduler(scheduler2)\n",
    "tensorboard = TensorBoard(log_dir=f'{notebook_output_prefix}logs/', histogram_freq=1, write_graph=True,\n",
    "                          write_images=True)\n",
    "checkpointer_loss = ModelCheckpoint(monitor='val_loss', filepath= path_prefix + '_loss.weights.h5', verbose=1, save_best_only=True, save_weights_only=True)\n",
    "checkpointer_acc = ModelCheckpoint(monitor='val_accuracy', filepath= path_prefix+ '_acc.weights.h5', verbose=1, save_best_only=True, save_weights_only=True)\n",
    "checkpointer_train_loss = ModelCheckpoint(monitor='loss', filepath= path_prefix + '_trainloss.weights.h5', verbose=1, save_best_only=True, save_weights_only=True)\n",
    "csv_logger = CSVLogger(path_prefix + '_training.log', append=True)\n",
    "tensorboard.set_model(model_nomb)\n",
    "\n",
    "history1 = model_nomb.fit(training_generator, \n",
    "          epochs=sum(epoch_list),\n",
    "          verbose=1,\n",
    "          shuffle=False,\n",
    "          validation_data=validation_generator,\n",
    "          callbacks=[tensorboard,checkpointer_loss,checkpointer_acc,checkpointer_train_loss,lr_callback,csv_logger])\n",
    "\n",
    "init_lr = 1e-5\n",
    "lr_callback = keras.callbacks.LearningRateScheduler(scheduler3)\n",
    "model_nomb.compile(loss='categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(learning_rate=init_lr), metrics=['accuracy',tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])\n",
    "history2 = model_nomb.fit(training_generator, \n",
    "          epochs=30,\n",
    "          verbose=1,\n",
    "          shuffle=False,\n",
    "          validation_data=validation_generator,\n",
    "          callbacks=[tensorboard,checkpointer_loss,checkpointer_acc,checkpointer_train_loss,lr_callback,csv_logger])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_nomb.save_weights(f\"{notebook_output_prefix}/model_weights/{MODEL_NAME}.weights.h5\")\n",
    "model_nomb.save(f\"{notebook_output_prefix}/full_model/{MODEL_NAME}.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The More Blocks Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More Blocks Model\n",
    "MODEL_NAME = \"MoreBlocksModel\"\n",
    "path_prefix = f'{notebook_output_prefix}{MODEL_NAME}'\n",
    "\n",
    "attention_model = AttentionModelTrunk(name='Co3_2x6head_4layer_NoEmbeddingTransformer',\n",
    "                             num_heads=2, head_size=6, ff_dim=64, num_layers=4, dropout=0.1)\n",
    "model_moreblocks = build_moreblocks_model((30,3), attention_model, n_classes, [512], 0.01, 0.01)\n",
    "\n",
    "training_generator, validation_generator = get_data_generator(training_exclu_x, training_exclu_y, val_x, val_y, n_classes, normalize(\"minmax\", \"cesnet\"),\n",
    "                                   batch_size = 64,feature_select=[0,1,2], dim =(30), n_channels = 3)\n",
    "\n",
    "dummy_init_lr = 1e-8\n",
    "model_moreblocks.compile(loss='categorical_crossentropy', \n",
    "                          optimizer=tf.keras.optimizers.Adam(learning_rate=dummy_init_lr), \\\n",
    "                          metrics=['accuracy',tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])\n",
    "\n",
    "epoch_list = [3,1,2,10]\n",
    "scheduler2 = scheduler2_factory(epoch_list, [1e-3, 1e-4, 1e-5,1e-6])\n",
    "\n",
    "lr_callback = keras.callbacks.LearningRateScheduler(scheduler2)\n",
    "tensorboard = TensorBoard(log_dir=f'{notebook_output_prefix}logs/', histogram_freq=1, write_graph=True,\n",
    "                          write_images=True)\n",
    "checkpointer_loss = ModelCheckpoint(monitor='val_loss', filepath= path_prefix + '_loss.weights.h5', verbose=1, save_best_only=True, save_weights_only=True)\n",
    "checkpointer_acc = ModelCheckpoint(monitor='val_accuracy', filepath= path_prefix+ '_acc.weights.h5', verbose=1, save_best_only=True, save_weights_only=True)\n",
    "checkpointer_train_loss = ModelCheckpoint(monitor='loss', filepath= path_prefix + '_trainloss.weights.h5', verbose=1, save_best_only=True, save_weights_only=True)\n",
    "csv_logger = CSVLogger(path_prefix + '_training.log', append=True)\n",
    "tensorboard.set_model(model_moreblocks)\n",
    "\n",
    "history = model_moreblocks.fit(training_generator, \n",
    "          epochs=15,\n",
    "          verbose=1,\n",
    "          shuffle=False,\n",
    "          validation_data=validation_generator,\n",
    "          callbacks=[tensorboard,checkpointer_loss,checkpointer_acc,checkpointer_train_loss,lr_callback,csv_logger])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_moreblocks.save_weights(f\"{notebook_output_prefix}/model_weights/{MODEL_NAME}.weights.h5\")\n",
    "model_moreblocks.save(f\"{notebook_output_prefix}/full_model/{MODEL_NAME}.keras\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPU TESTING",
   "language": "python",
   "name": "tf_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
